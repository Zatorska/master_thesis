{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wgrywanie bibliotek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tbats import TBATS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ustawienia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output folders to save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the current folder where the notebook is located\n",
    "current_folder = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "images_output_folder = os.path.join(current_folder, \"../02_paper/out_figures/\")\n",
    "tables_output_folder = os.path.join(current_folder, \"../02_paper/out_tables/\")\n",
    "models_output_folder = os.path.join(current_folder, \"../../models/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wgranie danych\n",
    "* Get the path to the \"data.zip\" file\n",
    "* Unpacking the file\n",
    "* Uploading the CSV (both: **_data_** and **_data_dict_**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the \"data\" folder inside the repository\n",
    "data_folder = os.path.join(current_folder, \"..\", \"00_data\")\n",
    "\n",
    "# Get the path to the \"data.zip\" file inside the \"data\" folder\n",
    "data_zip_path = os.path.join(data_folder, \"data.zip\")\n",
    "\n",
    "# Check if both entsoe_country_file and entsoe_country_dict_file exist in the target location\n",
    "entsoe_country_file = os.path.join(data_folder, \"entsoe_country.csv\")\n",
    "entsoe_country_dict_file = os.path.join(data_folder, \"entsoe_country_dict.csv\")\n",
    "\n",
    "if not (os.path.exists(entsoe_country_file) and os.path.exists(entsoe_country_dict_file)):\n",
    "    # Extract the data.zip file\n",
    "    with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_folder)\n",
    "    \n",
    "# Read the CSV files and create DataFrames\n",
    "data = pd.read_csv(entsoe_country_file, sep=';')\n",
    "data_dict = pd.read_csv(entsoe_country_dict_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_tab_00 = data[data[\"Variable\"] == \"BZN_PL\"].dropna().head(5)\n",
    "sample_data_tab_00 = sample_data_tab_00.rename(columns={\n",
    "    'TotalLoad_Actual_MW': 'TotalLoad\\_Actual\\_MW',\n",
    "    'TotalLoad_Forecast_MW': 'TotalLoad\\_Forecast\\_MW'\n",
    "})\n",
    "sample_data_tab_00['Variable'] = sample_data_tab_00['Variable'].replace({'BZN_PL': 'BZN\\_PL'}, regex=True)\n",
    "(sample_data_tab_00).style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_01.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba wierszy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3811848"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych (część 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "\n",
    "# Country name mapping\n",
    "data['CountryCode'] = data['Variable'].map(lambda x: x.lstrip('BZN_'))\n",
    "data = pd.merge(data, data_dict, on=\"CountryCode\")\n",
    "data = data.drop(['Variable', 'CountryCode'], axis=1)\n",
    "\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_dict).style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_02.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data only for Romania from 2021-01-31 00:15:00\n",
    "romania_data = data[(data['Country'] == 'Romania') & (data['Timestamp'] >= '2021-01-31 00:15:00')]\n",
    "\n",
    "# Create a column with full hours (rounded down to the nearest hour)\n",
    "romania_data.loc[:, 'Timestamp'] = romania_data['Timestamp'].dt.floor('H')\n",
    "\n",
    "# Group the data by full hours and calculate the mean of four measurements\n",
    "aggregated_romania_data = romania_data.groupby('Timestamp')['TotalLoad_Actual_MW'].mean().reset_index()\n",
    "aggregated_romania_data[\"Country\"] = \"Romania\"\n",
    "\n",
    "# Select the indices of the original data for Romania that meet the conditions\n",
    "indices_to_remove = data[(data['Country'] == 'Romania') & (data['Timestamp'] >= '2021-01-31 00:15:00')].index\n",
    "\n",
    "# Remove the original data for Romania that meets the conditions\n",
    "data = data.drop(indices_to_remove)\n",
    "data = data.drop(columns=[\"TotalLoad_Forecast_MW\"])\n",
    "\n",
    "# Concatenate the updated data for Romania\n",
    "data = pd.concat([data, aggregated_romania_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_tab_03 = data[data[\"Country\"] == \"Poland\"].dropna().head(5)\n",
    "sample_data_tab_03['TotalLoad_Actual_MW'] = sample_data_tab_03['TotalLoad_Actual_MW'].apply(lambda x: '{:.2f}'.format(x))\n",
    "sample_data_tab_03 = sample_data_tab_03.rename(columns={\n",
    "    'TotalLoad_Actual_MW': 'TotalLoad\\_Actual\\_MW'\n",
    "})\n",
    "(sample_data_tab_03).style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_03.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza brakujących danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba wierszy po wstępnym przygotowaniu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2161177"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba wierszy z NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41902"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TotalLoad_Actual_MW'].replace(0, np.nan, inplace=True)\n",
    "data[\"TotalLoad_Actual_MW\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values_per_country = []\n",
    "\n",
    "# Unique countries in the 'Country' column\n",
    "countries = data['Country'].unique()\n",
    "\n",
    "# Loop through each country\n",
    "for country in countries:\n",
    "    # Select rows only for the current country\n",
    "    df_country = data[data['Country'] == country]\n",
    "    \n",
    "    # Number of all rows in the country\n",
    "    num_rows = len(df_country)\n",
    "    \n",
    "    # Number of NaN in the 'TotalLoad_Actual_MW' column\n",
    "    num_nan = df_country['TotalLoad_Actual_MW'].isna().sum()\n",
    "    \n",
    "    # Percentage of NaN for all rows in the country\n",
    "    percent_nan = (num_nan / num_rows) * 100\n",
    "    percent_nan = '{:.2f}'.format(percent_nan)\n",
    "    \n",
    "    # Add results to the list\n",
    "    nan_values_per_country.append({\n",
    "        'Country': country,\n",
    "        'NumRows': num_rows,\n",
    "        'NumNaN': num_nan,\n",
    "        'PercentNaN': percent_nan\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "nan_values_per_country = pd.DataFrame(nan_values_per_country)\n",
    "\n",
    "nan_values_per_country.style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_04.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_value_date_ranges(dataframe):\n",
    "    dataframe['Timestamp'] = pd.to_datetime(dataframe['Timestamp'])\n",
    "\n",
    "    # Creation the resulting DataFrame.\n",
    "    result = []\n",
    "\n",
    "    # Iteration over unique countries.\n",
    "    for country in dataframe['Country'].unique():\n",
    "        country_data = dataframe[dataframe[\"Country\"] == country][[\"Timestamp\", \"TotalLoad_Actual_MW\"]]  \n",
    "        country_data['Timestamp'] = pd.to_datetime(country_data['Timestamp'])\n",
    "\n",
    "        # Aggregation data to full days and sorting.\n",
    "        agg_df = country_data.resample('D', on='Timestamp').sum().reset_index()\n",
    "\n",
    "        for metric in [\"TotalLoad_Actual_MW\"]:  \n",
    "            start_date = None\n",
    "            end_date = None\n",
    "\n",
    "            for index, row in agg_df.iterrows():\n",
    "                if row[metric] == 0.00:\n",
    "                    if start_date is None:\n",
    "                        start_date = row['Timestamp']\n",
    "                elif start_date is not None:\n",
    "                    end_date = row['Timestamp']\n",
    "                    number_of_days = (end_date - start_date).days + 1\n",
    "                    result.append({'Country': country, 'Metric': metric, 'start_date': start_date, 'end_date': end_date, 'number_of_days': number_of_days})\n",
    "                    start_date = None\n",
    "                    end_date = None\n",
    "            \n",
    "            # Handling missing end_date - if we have reached the last record\n",
    "            if end_date is None and start_date is not None:\n",
    "                end_date = agg_df['Timestamp'].iloc[-1]\n",
    "                number_of_days = (end_date - start_date).days + 1\n",
    "                result.append({'Country': country, 'Metric': metric, 'start_date': start_date, 'end_date': end_date, 'number_of_days': number_of_days})\n",
    "\n",
    "    missing_value_date_ranges = pd.DataFrame(result)\n",
    "    missing_value_date_ranges = missing_value_date_ranges.sort_values(by=['Country', 'Metric']).reset_index(drop=True)\n",
    "\n",
    "    missing_value_date_ranges.drop(columns=['Metric'], inplace=True) \n",
    "    \n",
    "    return missing_value_date_ranges\n",
    "\n",
    "missing_value_date_ranges = find_missing_value_date_ranges(data)\n",
    "\n",
    "column_mapping = {\n",
    "    'Country': 'Country',\n",
    "    'start_date': 'Start Date',\n",
    "    'end_date': 'End Date',\n",
    "    'number_of_days': 'Number of Days'\n",
    "}\n",
    "\n",
    "missing_value_date_ranges.columns = [column_mapping[col] for col in missing_value_date_ranges.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_date_ranges['Start Date'] = missing_value_date_ranges['Start Date'].dt.date\n",
    "missing_value_date_ranges['End Date'] = missing_value_date_ranges['End Date'].dt.date\n",
    "\n",
    "missing_value_date_ranges.style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_05.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykresy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przygotowanie danych do wykresów (agregacja do tygodnia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepared = data.set_index('Timestamp')\n",
    "\n",
    "def remove_first_last(group):\n",
    "    return group.iloc[1:-1]\n",
    "\n",
    "# By default, in pandas, a week is defined as a calendar week starting from Monday and ending on Sunday.\n",
    "# If a week spans across two years, it will be assigned to the year in which the majority of weekdays fall.\n",
    "weekly_data = data_prepared.groupby('Country')[['TotalLoad_Actual_MW']].resample('W').sum().reset_index()\n",
    "weekly_data = weekly_data.rename(columns={'Timestamp': 'Date'})\n",
    "weekly_data = weekly_data.groupby('Country').apply(remove_first_last).reset_index(drop=True)\n",
    "\n",
    "sample_data_tab_06 = weekly_data.head(5)\n",
    "sample_data_tab_06.loc[:, 'TotalLoad_Actual_MW'] = sample_data_tab_06['TotalLoad_Actual_MW'].apply(lambda x: '{:.2f}'.format(x))\n",
    "sample_data_tab_06 = sample_data_tab_06.rename(columns={\n",
    "    'TotalLoad_Actual_MW': 'TotalLoad\\_Actual\\_MW'\n",
    "})\n",
    "(sample_data_tab_06).style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_06.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _TotalLoad_Actual_MW dla kraju **przed imputacją**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = weekly_data['Country'].unique()\n",
    "\n",
    "for country in country_list:\n",
    "    electricity_consumption_per_country = weekly_data[weekly_data['Country'] == country].copy()\n",
    "    electricity_consumption_per_country['Date'] = pd.to_datetime(electricity_consumption_per_country['Date'])\n",
    "    electricity_consumption_per_country['TotalLoad_Actual_MW'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Find the index of the first non-NaN value in the 'TotalLoad_Actual_MW' column\n",
    "    first_non_nan_index = electricity_consumption_per_country['TotalLoad_Actual_MW'].first_valid_index()\n",
    "\n",
    "    # Trim the DataFrame from that position\n",
    "    if first_non_nan_index is not None:\n",
    "        electricity_consumption_per_country = electricity_consumption_per_country.loc[first_non_nan_index:]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.ylabel(\"Electricity Consumption (Terawatts)\", fontsize=14)\n",
    "    plt.title(f\"Actual Electricity Consumption in Terawatts for Country: {country}\", fontsize=16)\n",
    "    \n",
    "    formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "    formatter.set_scientific(False)\n",
    "    formatter.set_powerlimits((-6, 6))\n",
    "    plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    plt.plot(electricity_consumption_per_country[\"Date\"], electricity_consumption_per_country[\"TotalLoad_Actual_MW\"], color='steelblue')\n",
    "    \n",
    "    plt.xlim(electricity_consumption_per_country[\"Date\"].iloc[0], electricity_consumption_per_country[\"Date\"].iloc[-1])\n",
    "    \n",
    "    ticks = plt.gca().get_yticks()\n",
    "    tick_labels = [f'{int(tick) / 1000000:.1f}' for tick in ticks]\n",
    "    plt.gca().yaxis.set_major_locator(ticker.FixedLocator(ticks))\n",
    "    plt.gca().set_yticklabels(tick_labels)\n",
    "\n",
    "    output_file_path = os.path.join(images_output_folder, f\"actual_electricity_consumption_{country}.png\")\n",
    "    if not os.path.exists(output_file_path):\n",
    "        plt.savefig(output_file_path)\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputacja\n",
    "\n",
    "Imputation was performed using a weighted average with a 7-day window.\n",
    "\n",
    "Currently, due to a significant amount of missing data for Cyprus, I am refraining from performing imputation for this country. This method is not efficient for Cyprus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for imputation\n",
    "def impute_missing_values(row):\n",
    "    if pd.isnull(row['TotalLoad_Imputed_MW']):\n",
    "        # Select the last 30 measurements for the same weekday and hour\n",
    "        recent_data = country_data[\n",
    "            (country_data['Timestamp'].dt.weekday == row['Timestamp'].weekday()) &\n",
    "            (country_data['Timestamp'].dt.hour == row['Timestamp'].hour)\n",
    "        ].tail(30)\n",
    "\n",
    "        # Fill the missing field with the mean of the last 30 measurements\n",
    "        imputed_value = recent_data['TotalLoad_Actual_MW'].mean()\n",
    "        return imputed_value\n",
    "    else:\n",
    "        return row['TotalLoad_Imputed_MW']\n",
    "\n",
    "# Create a copy of the DataFrame data\n",
    "data_imputed = data.copy()\n",
    "\n",
    "# Add a column TotalLoad_Imputed_MW with the original data\n",
    "data_imputed['TotalLoad_Imputed_MW'] = data_imputed['TotalLoad_Actual_MW']\n",
    "\n",
    "# Unique countries in the DataFrame\n",
    "unique_countries = data['Country'].unique()\n",
    "\n",
    "# Loop for each country\n",
    "for country in unique_countries:\n",
    "    # Select data only for the specific country\n",
    "    country_data = data[data['Country'] == country]\n",
    "    # Find the index of the first non-NaN value in the 'TotalLoad_Actual_MW' column\n",
    "    first_non_nan_index = country_data['TotalLoad_Actual_MW'].first_valid_index()\n",
    "    # Trim the DataFrame to that position\n",
    "    if first_non_nan_index is not None:\n",
    "        country_data = country_data.loc[first_non_nan_index:]\n",
    "\n",
    "    # Sort the DataFrame by the Timestamp column\n",
    "    country_data = country_data.sort_values(by='Timestamp')\n",
    "\n",
    "    # Apply the imputation function and save the results in the TotalLoad_Imputed_MW column\n",
    "    data_imputed.loc[data_imputed['Country'] == country, 'TotalLoad_Imputed_MW'] = data_imputed[data_imputed['Country'] == country].apply(impute_missing_values, axis=1)\n",
    "\n",
    "# Restore the original order of the DataFrame\n",
    "data_imputed = data_imputed.sort_values(by='Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykresy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the index to 'Timestamp'.\n",
    "data_imputed.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Resampling and grouping by weeks\n",
    "weekly_imputed_data = data_imputed.groupby('Country')[['TotalLoad_Actual_MW', 'TotalLoad_Imputed_MW']].resample('W').sum().reset_index()  \n",
    "weekly_imputed_data = weekly_imputed_data.rename(columns={'Timestamp': 'Date'})\n",
    "weekly_imputed_data = weekly_imputed_data.groupby('Country').apply(remove_first_last).reset_index(drop=True)\n",
    "weekly_imputed_data = weekly_imputed_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TotalLoad_Actual_MW per country **after imputation**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = weekly_imputed_data['Country'].unique()\n",
    "for country in country_list:\n",
    "    electricity_consumption_per_country = weekly_imputed_data[weekly_imputed_data['Country'] == country]\n",
    "    electricity_consumption_per_country = electricity_consumption_per_country.reset_index(drop=True)\n",
    "    electricity_consumption_per_country.drop(columns=\"index\", inplace=True)\n",
    "\n",
    "    electricity_consumption_per_country = weekly_imputed_data[weekly_imputed_data['Country'] == country].copy()\n",
    "    electricity_consumption_per_country['Date'] = pd.to_datetime(electricity_consumption_per_country['Date'])\n",
    "    electricity_consumption_per_country['TotalLoad_Actual_MW'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Find the index of the first non-NaN value in the 'TotalLoad_Actual_MW' column\n",
    "    first_non_nan_index = electricity_consumption_per_country['TotalLoad_Actual_MW'].first_valid_index()\n",
    "\n",
    "    # Trim the DataFrame from that position\n",
    "    if first_non_nan_index is not None:\n",
    "        electricity_consumption_per_country = electricity_consumption_per_country.loc[first_non_nan_index:]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.ylabel(\"Electricity Consumption (Terawatts)\", fontsize=14)\n",
    "    plt.title(f\"Imputed Electricity Consumption in Terawatts for Country: {country}\", fontsize=16)\n",
    "    \n",
    "    formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "    formatter.set_scientific(False)\n",
    "    formatter.set_powerlimits((-6, 6))\n",
    "    plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    plt.plot(electricity_consumption_per_country[\"Date\"], electricity_consumption_per_country[\"TotalLoad_Actual_MW\"], color='firebrick', label='Actual')\n",
    "    plt.plot(electricity_consumption_per_country[\"Date\"], electricity_consumption_per_country[\"TotalLoad_Imputed_MW\"], color='steelblue', label='Imputed')\n",
    "    \n",
    "    plt.xlim(electricity_consumption_per_country[\"Date\"].iloc[0], electricity_consumption_per_country[\"Date\"].iloc[-1])\n",
    "    plt.legend()\n",
    "    \n",
    "    ticks = plt.gca().get_yticks()\n",
    "    tick_labels = [f'{int(tick) / 1000000:.1f}' for tick in ticks]\n",
    "    plt.gca().yaxis.set_major_locator(ticker.FixedLocator(ticks))\n",
    "    plt.gca().set_yticklabels(tick_labels)\n",
    "\n",
    "    output_file_path = os.path.join(images_output_folder, f\"imputed_electricity_consumption_{country}.png\")\n",
    "    if not os.path.exists(output_file_path):\n",
    "        plt.savefig(output_file_path)\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ostateczne przygotowanie danych do trenowania/testowania modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_the_model = weekly_imputed_data[['Country', 'Date', 'TotalLoad_Imputed_MW']]\n",
    "data_for_the_model.loc[:, 'Date'] = pd.to_datetime(data_for_the_model['Date']).dt.date\n",
    "data_for_the_model.to_csv('nazwa_pliku.csv', index=False)\n",
    "sample_data_tab_07 = data_for_the_model[data_for_the_model[\"Country\"] == \"Poland\"].head(5)\n",
    "sample_data_tab_07['TotalLoad_Imputed_MW'] = sample_data_tab_07['TotalLoad_Imputed_MW'].apply(lambda x: '{:.2f}'.format(x))\n",
    "sample_data_tab_07 = sample_data_tab_07.rename(columns={\n",
    "    'TotalLoad_Imputed_MW': 'TotalLoad\\_Imputed\\_MW'\n",
    "})\n",
    "(sample_data_tab_07).style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_07.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDYKCJE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = sorted(data_for_the_model['Country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRYKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y, y_pred):\n",
    "    mape = np.mean(np.abs((y - y_pred)/y))*100\n",
    "    return round(mape, 2)\n",
    "\n",
    "def ME(y, y_pred):\n",
    "    me = np.mean(y_pred - y)\n",
    "    return round(me, 2)\n",
    "\n",
    "def lstm_me(y_true, y_pred):\n",
    "    return K.mean(y_pred - y_true, axis=-1)\n",
    "\n",
    "def RMSE(MSE):\n",
    "    rmse = math.sqrt(MSE)\n",
    "    return round(rmse, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Szukanie optymalnych parametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sarima_split_best_params_search_fit_predict_plot()** function was employed to discover the optimal parameters for SARIMA models for each country, as well as to evaluate the model performance using the selected parameters. Commented out to avoid re-searching parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_split_best_params_search_fit_predict_plot(country_name, data):\n",
    "    \n",
    "    dataset = data.values\n",
    "    if country_name == \"Cyprus\":\n",
    "        train_data = country_data[country_data.index > '2016-09-21']\n",
    "    train_data = data[data.index <= '2019-12-31']\n",
    "    test_data = data[data.index >= '2020-01-01']\n",
    "    \n",
    "    p = range(0, 2)\n",
    "    d = range(0, 2)\n",
    "    q = range(0, 2)\n",
    "    P = range(0, 2)\n",
    "    D = range(1, 2)\n",
    "    Q = range(0, 2)\n",
    "    s = 12\n",
    "\n",
    "    best_aic = float(\"inf\")\n",
    "    best_params = None\n",
    "\n",
    "    for p_val in p:\n",
    "        for d_val in d:\n",
    "            for q_val in q:\n",
    "                for P_val in P:\n",
    "                    for D_val in D:\n",
    "                        for Q_val in Q:\n",
    "                            try:\n",
    "                                model = SARIMAX(train_data, order=(p_val, d_val, q_val), seasonal_order=(P_val, D_val, Q_val, s))\n",
    "                                fit_model = model.fit()\n",
    "                                aic = fit_model.aic\n",
    "                                if aic < best_aic:\n",
    "                                    best_aic = aic\n",
    "                                    best_params = (p_val, d_val, q_val, P_val, D_val, Q_val)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "    print(f\"Best SARIMA parameters for {country_name}:\", best_params)\n",
    "\n",
    "    best_params_results = {\n",
    "        'country': country_name,\n",
    "        'best_params': best_params,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        p_val, d_val, q_val, P_val, D_val, Q_val = best_params\n",
    "        s = 52 \n",
    "        model = SARIMAX(train_data, order=(p_val, d_val, q_val), seasonal_order=(P_val, D_val, Q_val, s))\n",
    "        fit_model = model.fit()\n",
    "        yhat = fit_model.predict(start=len(train_data), end=(len(dataset)-1))\n",
    "    \n",
    "        model_filename = f\"sarima_model_{country_name}.pkl\"\n",
    "        model_file_path = os.path.join(models_output_folder, model_filename)\n",
    "\n",
    "        if not os.path.exists(model_file_path):\n",
    "            with open(model_file_path, 'wb') as model_file:\n",
    "                pickle.dump(fit_model, model_file)\n",
    "\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.ylabel(\"Electricity Consumption (Terawatts)\", fontsize=14)\n",
    "        plt.title(f\"SARIMA Prediction for Electricity Consumption in Terawatts for Country: {country_name}\", fontsize=16)\n",
    "\n",
    "        yhat_df = pd.DataFrame(yhat)\n",
    "        country_data_subset = country_data.iloc[1:-1]\n",
    "        yhat_df_subset = yhat_df.iloc[1:-1]\n",
    "    \n",
    "        formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(False)  \n",
    "        formatter.set_powerlimits((-6, 6))  \n",
    "        plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "        plt.plot(yhat_df_subset, color=\"firebrick\", label='Predicted')\n",
    "        plt.plot(country_data_subset, color='steelblue', label='Actual')\n",
    "    \n",
    "        plt.xlim(country_data_subset.index[0], country_data_subset.index[-1])\n",
    "        plt.legend()\n",
    "    \n",
    "        # Changing the Y-axis value labels from 5000000 to 5.0\n",
    "        plt.gca().set_yticklabels([f'{int(tick) / 1000000:.1f}' for tick in plt.gca().get_yticks()])\n",
    "\n",
    "        output_file_path = os.path.join(images_output_folder, f\"sarima_predictions_{country}.png\")\n",
    "        if not os.path.exists(output_file_path):\n",
    "            plt.savefig(output_file_path)\n",
    "    \n",
    "        plt.close()\n",
    "    \n",
    "        MAPE_metric = MAPE(test_data, yhat)\n",
    "        ME_metric = ME(test_data, yhat)\n",
    "        MAE_metric = round(mean_absolute_error(test_data, yhat), 2)\n",
    "        MSE_metric = round(mean_squared_error(test_data, yhat), 2)\n",
    "        RMSE_metric = RMSE(MSE_metric)\n",
    "\n",
    "        results = {\n",
    "            'Model': 'sarima',\n",
    "            'Country': country_name,\n",
    "            'MAPE': MAPE_metric,\n",
    "            'ME': ME_metric,\n",
    "            'MAE': MAE_metric,\n",
    "            'MSE': MSE_metric,\n",
    "            'RMSE': RMSE_metric\n",
    "        }\n",
    "    \n",
    "        return best_params_results, results, yhat_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for country {country_name}: {e}\")\n",
    "\n",
    "best_params_list = []\n",
    "results_list = []\n",
    "sarima_predictions = pd.DataFrame()\n",
    "\n",
    "for country in data_for_the_model['Country'].unique():\n",
    "    try: \n",
    "        print(f'Evaluation for country: {country}')\n",
    "        country_data = data_for_the_model[data_for_the_model['Country'] == country].set_index('Date').asfreq('W')\n",
    "        best_params, results, yhat_df = sarima_split_best_params_search_fit_predict_plot(country, country_data[\"TotalLoad_Imputed_MW\"])\n",
    "        yhat_df = yhat_df.rename(columns={'predicted_mean': country})\n",
    "        sarima_predictions = pd.concat([sarima_predictions, yhat_df], axis=1)\n",
    "        results_list.append(results)\n",
    "        best_params_list.append(best_params)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for country {country}: {e}\")\n",
    "\n",
    "\n",
    "sarima_results = pd.DataFrame(results_list)\n",
    "sarima_best_params_df = pd.DataFrame(best_params_list)\n",
    "sarima_best_params_df[['p', 'd', 'q', 'P', 'D', 'Q']] = pd.DataFrame(sarima_best_params_df['Params'].tolist(), index=sarima_best_params_df.index)\n",
    "sarima_best_params_df = sarima_best_params_df.drop('Params', axis=1)\n",
    "(sarima_best_params_df).style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_08.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TBATS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Szukanie optymalnych parametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tbats_split_best_params_search_fit_predict()** function was employed to discover the optimal parameters for TBATS models for each country, as well as to evaluate the model performance using the selected parameters. Commented out to avoid re-searching parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tbats_split_best_params_search_fit_predict(country_name, data):\n",
    "    \n",
    "    dataset = data.values\n",
    "    if country_name == \"Cyprus\":\n",
    "        train_data = country_data[country_data.index > '2016-09-21']\n",
    "    train_data = data[data.index <= '2019-12-31']\n",
    "    test_data = data[data.index >= '2020-01-01']\n",
    "    \n",
    "    seasonal_periods = 52\n",
    "    use_arma_errors = True\n",
    "    use_box_cox_options = [True, False]\n",
    "    use_trend_options = [True, False]\n",
    "    n_jobs_option = os.cpu_count()\n",
    "    use_damped_trend = True\n",
    "    \n",
    "    best_aic = float(\"inf\")\n",
    "    best_params = None\n",
    "    best_result_dict = None\n",
    "\n",
    "    for use_box_cox in use_box_cox_options:\n",
    "        for use_trend in use_trend_options:\n",
    "            try:\n",
    "                model = TBATS(seasonal_periods=[seasonal_periods],\n",
    "                              use_arma_errors=use_arma_errors,\n",
    "                              use_box_cox=use_box_cox,\n",
    "                              use_trend=use_trend,\n",
    "                              n_jobs=n_jobs_option,\n",
    "                              use_damped_trend=use_damped_trend\n",
    "                             )\n",
    "                fit_model = model.fit(train_data)\n",
    "                aic = fit_model.aic\n",
    "                if aic < best_aic:\n",
    "                    best_aic = aic\n",
    "                    best_params = (\n",
    "                        seasonal_periods, \n",
    "                        use_arma_errors, \n",
    "                        use_box_cox,\n",
    "                        use_trend, \n",
    "                        n_jobs_option,\n",
    "                        use_damped_trend\n",
    "                        )\n",
    "                    best_result_dict = {\n",
    "                    \"country\": country_name,\n",
    "                    \"seasonal_period\": seasonal_periods,\n",
    "                    \"use_arma_errors\": use_arma_errors,\n",
    "                    \"use_box_cox\": use_box_cox,\n",
    "                    \"use_trend\": use_trend,\n",
    "                    \"use_damped_trend\": use_damped_trend\n",
    "                }\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    tbats_best_params[country_name] = best_result_dict\n",
    "\n",
    "    print(f'Best params for country {country_name}: {best_params}')\n",
    "    \n",
    "    best_params_results = {\n",
    "        'country': country_name,\n",
    "        'best_params': best_params,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        (\n",
    "        seasonal_period, \n",
    "        use_arma_errors, \n",
    "        use_box_cox,\n",
    "        use_trend, \n",
    "        n_jobs_option,\n",
    "        use_damped_trend\n",
    "        ) = best_params\n",
    "    \n",
    "        model = TBATS(seasonal_periods=[seasonal_period],\n",
    "                                        use_arma_errors=use_arma_errors,\n",
    "                                        use_box_cox=use_box_cox,\n",
    "                                        use_trend=use_trend,\n",
    "                                        n_jobs=n_jobs_option,\n",
    "                                        use_damped_trend=use_damped_trend\n",
    "                                        )\n",
    "        fit_model = model.fit(train_data)\n",
    "        yhat = fit_model.forecast(steps=len(test_data))\n",
    "\n",
    "        model_filename = f\"tbats_model_{country_name}.pkl\"\n",
    "        model_file_path = os.path.join(models_output_folder, model_filename)\n",
    "\n",
    "        if not os.path.exists(model_file_path):\n",
    "            with open(model_file_path, 'wb') as model_file:\n",
    "                pickle.dump(fit_model, model_file)\n",
    "\n",
    "        start_date = '2020-01-07'\n",
    "        end_date = '2021-09-05'\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='7D')\n",
    "\n",
    "        yhat_df = pd.DataFrame(yhat, index=date_range)\n",
    "        yhat_df_subset = yhat_df.iloc[1:-1]\n",
    "\n",
    "        country_data_subset = country_data.iloc[1:-1] \n",
    "\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.ylabel(\"Electricity Consumption (Terawatts)\", fontsize=14)\n",
    "        plt.title(f\"TBATS Prediction for Electricity Consumption in Terawatts for Country: {country_name}\", fontsize=16)\n",
    "    \n",
    "        formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(False)\n",
    "        formatter.set_powerlimits((-6, 6))\n",
    "        plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "        plt.plot(yhat_df_subset, color=\"firebrick\", label='Predicted')\n",
    "        plt.plot(country_data_subset, color='steelblue', label='Actual')\n",
    "    \n",
    "        plt.xlim(country_data_subset.index[0], country_data_subset.index[-1])\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.gca().set_yticklabels([f'{int(tick) / 1000000:.1f}' for tick in plt.gca().get_yticks()])\n",
    "\n",
    "        output_file_path = os.path.join(images_output_folder, f\"tbats_predictions_{country}.png\")\n",
    "        if not os.path.exists(output_file_path):\n",
    "            plt.savefig(output_file_path)\n",
    "    \n",
    "        plt.close()\n",
    "\n",
    "        MAPE_metric = MAPE(test_data, yhat)\n",
    "        ME_metric = ME(test_data, yhat)\n",
    "        MAE_metric = round(mean_absolute_error(test_data, yhat), 2)\n",
    "        MSE_metric = round(mean_squared_error(test_data, yhat), 2)\n",
    "        RMSE_metric = RMSE(MSE_metric)\n",
    "\n",
    "        results = {\n",
    "            'Model': 'tbats',\n",
    "            'Country': country_name,\n",
    "            'MAPE': MAPE_metric,\n",
    "            'ME': ME_metric,\n",
    "            'MAE': MAE_metric,\n",
    "            'MSE': MSE_metric,\n",
    "            'RMSE': RMSE_metric\n",
    "        }\n",
    "\n",
    "        return best_params_results, results, yhat_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for country {country}: {e}\")\n",
    "\n",
    "best_params_list = []\n",
    "results_list = []\n",
    "tbats_predictions = pd.DataFrame()\n",
    "\n",
    "for country in data_for_the_model['Country'].unique():\n",
    "    try: \n",
    "        print(f'Evaluation for country: {country}')\n",
    "        country_data = data_for_the_model[data_for_the_model['Country'] == country].set_index('Date').asfreq('W')\n",
    "        best_params, results, yhat_df = tbats_split_best_params_search_fit_predict(country, country_data[\"TotalLoad_Imputed_MW\"])\n",
    "        yhat_df = yhat_df.rename(columns={0: country})\n",
    "        tbats_predictions = pd.concat([tbats_predictions, yhat_df], axis=1)\n",
    "        best_params_list.append(best_params)\n",
    "        results_list.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for country {country}: {e}\")\n",
    "\n",
    "tbats_results = pd.DataFrame(results_list)\n",
    "tbats_best_params_df = pd.DataFrame(best_params_list))\n",
    "\n",
    "# Splitting the 'Params' column into separate columns.\n",
    "tbats_best_params_df[['seasonal\\_period', \n",
    "    'use\\_arma\\_errors', \n",
    "    'use\\_box\\_cox',\n",
    "    'use\\_trend', \n",
    "    'n\\_jobs\\_option',\n",
    "    'use\\_damped\\_trend']] = pd.DataFrame(tbats_best_params_df['Params'].tolist(), index=tbats_best_params_df.index)\n",
    "tbats_best_params_df = tbats_best_params_df.drop(['Params', 'n\\_jobs\\_option'], axis=1)\n",
    "tbats_best_params_df.loc[tbats_best_params_df['use\\_trend'] == False, 'use\\_damped\\_trend'] = False\n",
    "tbats_best_params_df\n",
    "(tbats_best_params_df).style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_09.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trenowanie, predykcja, wykresy i ewaluacja modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trenowanie, predykcja, wykresy i ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for country: Cyprus\n",
      "Epoch 1/150\n",
      "240/240 - 7s - loss: 0.1084 - mape: 209.8337 - lstm_me: 0.0046 - 7s/epoch - 30ms/step\n",
      "Epoch 2/150\n",
      "240/240 - 2s - loss: 0.0533 - mape: 395.4050 - lstm_me: 0.0063 - 2s/epoch - 8ms/step\n",
      "Epoch 3/150\n",
      "240/240 - 2s - loss: 0.0433 - mape: 89.6225 - lstm_me: 2.6964e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 4/150\n",
      "240/240 - 2s - loss: 0.0373 - mape: 221.7097 - lstm_me: -2.6667e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 5/150\n",
      "240/240 - 3s - loss: 0.0339 - mape: 152.5113 - lstm_me: 0.0012 - 3s/epoch - 11ms/step\n",
      "Epoch 6/150\n",
      "240/240 - 3s - loss: 0.0338 - mape: 181.9084 - lstm_me: 0.0030 - 3s/epoch - 13ms/step\n",
      "Epoch 7/150\n",
      "240/240 - 2s - loss: 0.0319 - mape: 93.0480 - lstm_me: 8.3453e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 8/150\n",
      "240/240 - 2s - loss: 0.0319 - mape: 72.5710 - lstm_me: -8.9240e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 9/150\n",
      "240/240 - 2s - loss: 0.0347 - mape: 140.1977 - lstm_me: 0.0022 - 2s/epoch - 9ms/step\n",
      "Epoch 10/150\n",
      "240/240 - 2s - loss: 0.0301 - mape: 176.2843 - lstm_me: 0.0029 - 2s/epoch - 10ms/step\n",
      "Epoch 11/150\n",
      "240/240 - 2s - loss: 0.0332 - mape: 146.7486 - lstm_me: -5.4600e-05 - 2s/epoch - 8ms/step\n",
      "Epoch 12/150\n",
      "240/240 - 2s - loss: 0.0299 - mape: 267.6797 - lstm_me: -2.3435e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 13/150\n",
      "240/240 - 2s - loss: 0.0299 - mape: 241.7950 - lstm_me: 0.0038 - 2s/epoch - 8ms/step\n",
      "Epoch 14/150\n",
      "240/240 - 2s - loss: 0.0292 - mape: 107.4354 - lstm_me: 0.0090 - 2s/epoch - 8ms/step\n",
      "Epoch 15/150\n",
      "240/240 - 2s - loss: 0.0304 - mape: 185.6869 - lstm_me: -7.9855e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 16/150\n",
      "240/240 - 2s - loss: 0.0291 - mape: 164.7832 - lstm_me: 0.0069 - 2s/epoch - 8ms/step\n",
      "Epoch 17/150\n",
      "240/240 - 2s - loss: 0.0295 - mape: 105.5956 - lstm_me: 0.0025 - 2s/epoch - 8ms/step\n",
      "Epoch 18/150\n",
      "240/240 - 2s - loss: 0.0273 - mape: 136.6054 - lstm_me: 7.7198e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 19/150\n",
      "240/240 - 3s - loss: 0.0286 - mape: 159.3539 - lstm_me: -6.0404e-04 - 3s/epoch - 14ms/step\n",
      "Epoch 20/150\n",
      "240/240 - 2s - loss: 0.0286 - mape: 113.3236 - lstm_me: -1.9931e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 21/150\n",
      "240/240 - 2s - loss: 0.0306 - mape: 187.2556 - lstm_me: 0.0024 - 2s/epoch - 8ms/step\n",
      "Epoch 22/150\n",
      "240/240 - 2s - loss: 0.0293 - mape: 226.9834 - lstm_me: 0.0034 - 2s/epoch - 8ms/step\n",
      "Epoch 23/150\n",
      "240/240 - 2s - loss: 0.0300 - mape: 182.4757 - lstm_me: 0.0025 - 2s/epoch - 8ms/step\n",
      "Epoch 24/150\n",
      "240/240 - 2s - loss: 0.0306 - mape: 197.7305 - lstm_me: -2.1839e-03 - 2s/epoch - 9ms/step\n",
      "Epoch 25/150\n",
      "240/240 - 2s - loss: 0.0286 - mape: 170.3716 - lstm_me: -3.6716e-04 - 2s/epoch - 9ms/step\n",
      "Epoch 26/150\n",
      "240/240 - 2s - loss: 0.0287 - mape: 187.1021 - lstm_me: 0.0033 - 2s/epoch - 8ms/step\n",
      "Epoch 27/150\n",
      "240/240 - 2s - loss: 0.0276 - mape: 155.5303 - lstm_me: -3.1094e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 28/150\n",
      "240/240 - 2s - loss: 0.0274 - mape: 109.3135 - lstm_me: 0.0041 - 2s/epoch - 8ms/step\n",
      "Epoch 29/150\n",
      "240/240 - 2s - loss: 0.0308 - mape: 162.6613 - lstm_me: -3.2203e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 30/150\n",
      "240/240 - 3s - loss: 0.0305 - mape: 219.7050 - lstm_me: -1.1647e-03 - 3s/epoch - 12ms/step\n",
      "Epoch 31/150\n",
      "240/240 - 2s - loss: 0.0282 - mape: 151.8352 - lstm_me: 0.0030 - 2s/epoch - 8ms/step\n",
      "Epoch 32/150\n",
      "240/240 - 4s - loss: 0.0306 - mape: 203.5428 - lstm_me: 0.0027 - 4s/epoch - 18ms/step\n",
      "Epoch 33/150\n",
      "240/240 - 3s - loss: 0.0291 - mape: 206.1462 - lstm_me: -4.8026e-03 - 3s/epoch - 11ms/step\n",
      "Epoch 34/150\n",
      "240/240 - 2s - loss: 0.0297 - mape: 133.5831 - lstm_me: 0.0069 - 2s/epoch - 7ms/step\n",
      "Epoch 35/150\n",
      "240/240 - 2s - loss: 0.0299 - mape: 166.5244 - lstm_me: -5.8278e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 36/150\n",
      "240/240 - 2s - loss: 0.0288 - mape: 213.0121 - lstm_me: 0.0024 - 2s/epoch - 7ms/step\n",
      "Epoch 37/150\n",
      "240/240 - 2s - loss: 0.0292 - mape: 186.0439 - lstm_me: 0.0029 - 2s/epoch - 9ms/step\n",
      "Epoch 38/150\n",
      "240/240 - 2s - loss: 0.0292 - mape: 148.6048 - lstm_me: -4.1608e-03 - 2s/epoch - 9ms/step\n",
      "Epoch 39/150\n",
      "240/240 - 2s - loss: 0.0313 - mape: 202.8669 - lstm_me: 0.0059 - 2s/epoch - 8ms/step\n",
      "Epoch 40/150\n",
      "240/240 - 2s - loss: 0.0270 - mape: 139.4072 - lstm_me: -2.6845e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 41/150\n",
      "240/240 - 2s - loss: 0.0287 - mape: 116.9001 - lstm_me: 0.0059 - 2s/epoch - 7ms/step\n",
      "Epoch 42/150\n",
      "240/240 - 2s - loss: 0.0294 - mape: 147.5562 - lstm_me: -1.1414e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 43/150\n",
      "240/240 - 2s - loss: 0.0264 - mape: 95.8797 - lstm_me: -1.9362e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 44/150\n",
      "240/240 - 2s - loss: 0.0261 - mape: 160.8338 - lstm_me: 5.1997e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 45/150\n",
      "240/240 - 2s - loss: 0.0261 - mape: 206.7614 - lstm_me: 4.8386e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 46/150\n",
      "240/240 - 2s - loss: 0.0274 - mape: 187.3536 - lstm_me: 0.0019 - 2s/epoch - 7ms/step\n",
      "Epoch 47/150\n",
      "240/240 - 2s - loss: 0.0269 - mape: 155.3691 - lstm_me: -4.4002e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 48/150\n",
      "240/240 - 2s - loss: 0.0250 - mape: 203.9756 - lstm_me: -5.1632e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 49/150\n",
      "240/240 - 2s - loss: 0.0267 - mape: 211.0187 - lstm_me: 0.0070 - 2s/epoch - 7ms/step\n",
      "Epoch 50/150\n",
      "240/240 - 2s - loss: 0.0254 - mape: 211.5501 - lstm_me: 0.0022 - 2s/epoch - 8ms/step\n",
      "Epoch 51/150\n",
      "240/240 - 2s - loss: 0.0268 - mape: 224.9424 - lstm_me: -2.8648e-03 - 2s/epoch - 9ms/step\n",
      "Epoch 52/150\n",
      "240/240 - 2s - loss: 0.0262 - mape: 157.9898 - lstm_me: -2.2334e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 53/150\n",
      "240/240 - 2s - loss: 0.0283 - mape: 155.0892 - lstm_me: 6.1607e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 54/150\n",
      "240/240 - 2s - loss: 0.0246 - mape: 306.0603 - lstm_me: -2.7579e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 55/150\n",
      "240/240 - 2s - loss: 0.0271 - mape: 185.7118 - lstm_me: 0.0073 - 2s/epoch - 8ms/step\n",
      "Epoch 56/150\n",
      "240/240 - 2s - loss: 0.0258 - mape: 166.2642 - lstm_me: -1.9828e-03 - 2s/epoch - 9ms/step\n",
      "Epoch 57/150\n",
      "240/240 - 2s - loss: 0.0260 - mape: 291.0952 - lstm_me: -1.7020e-03 - 2s/epoch - 9ms/step\n",
      "Epoch 58/150\n",
      "240/240 - 2s - loss: 0.0257 - mape: 206.9032 - lstm_me: -1.9496e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 59/150\n",
      "240/240 - 2s - loss: 0.0245 - mape: 201.4770 - lstm_me: -5.4761e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 60/150\n",
      "240/240 - 2s - loss: 0.0255 - mape: 166.3300 - lstm_me: -2.8041e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 61/150\n",
      "240/240 - 2s - loss: 0.0265 - mape: 228.9259 - lstm_me: 3.7914e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 62/150\n",
      "240/240 - 2s - loss: 0.0300 - mape: 216.1640 - lstm_me: 0.0010 - 2s/epoch - 8ms/step\n",
      "Epoch 63/150\n",
      "240/240 - 2s - loss: 0.0244 - mape: 176.9148 - lstm_me: 1.9644e-04 - 2s/epoch - 9ms/step\n",
      "Epoch 64/150\n",
      "240/240 - 2s - loss: 0.0231 - mape: 266.9784 - lstm_me: -1.0813e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 65/150\n",
      "240/240 - 2s - loss: 0.0250 - mape: 222.4605 - lstm_me: -2.0364e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 66/150\n",
      "240/240 - 2s - loss: 0.0249 - mape: 255.2973 - lstm_me: 0.0010 - 2s/epoch - 8ms/step\n",
      "Epoch 67/150\n",
      "240/240 - 2s - loss: 0.0242 - mape: 205.9695 - lstm_me: 0.0070 - 2s/epoch - 8ms/step\n",
      "Epoch 68/150\n",
      "240/240 - 2s - loss: 0.0225 - mape: 144.7678 - lstm_me: -3.5991e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 69/150\n",
      "240/240 - 2s - loss: 0.0235 - mape: 279.0109 - lstm_me: -1.5483e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 70/150\n",
      "240/240 - 2s - loss: 0.0248 - mape: 157.1230 - lstm_me: -5.1997e-04 - 2s/epoch - 9ms/step\n",
      "Epoch 71/150\n",
      "240/240 - 2s - loss: 0.0244 - mape: 200.3495 - lstm_me: 0.0060 - 2s/epoch - 8ms/step\n",
      "Epoch 72/150\n",
      "240/240 - 2s - loss: 0.0243 - mape: 162.4653 - lstm_me: -3.4755e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 73/150\n",
      "240/240 - 2s - loss: 0.0225 - mape: 183.6678 - lstm_me: -2.0578e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 74/150\n",
      "240/240 - 2s - loss: 0.0207 - mape: 224.8971 - lstm_me: -4.1566e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 75/150\n",
      "240/240 - 2s - loss: 0.0256 - mape: 199.5745 - lstm_me: 0.0018 - 2s/epoch - 7ms/step\n",
      "Epoch 76/150\n",
      "240/240 - 2s - loss: 0.0252 - mape: 225.0190 - lstm_me: -1.7036e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 77/150\n",
      "240/240 - 2s - loss: 0.0207 - mape: 243.7662 - lstm_me: 0.0070 - 2s/epoch - 7ms/step\n",
      "Epoch 78/150\n",
      "240/240 - 2s - loss: 0.0222 - mape: 241.1388 - lstm_me: -7.9004e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 79/150\n",
      "240/240 - 2s - loss: 0.0209 - mape: 155.4348 - lstm_me: -2.0419e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 80/150\n",
      "240/240 - 2s - loss: 0.0197 - mape: 271.6181 - lstm_me: -2.7406e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 81/150\n",
      "240/240 - 2s - loss: 0.0224 - mape: 60.6505 - lstm_me: 0.0052 - 2s/epoch - 7ms/step\n",
      "Epoch 82/150\n",
      "240/240 - 2s - loss: 0.0191 - mape: 150.9417 - lstm_me: -2.2876e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 83/150\n",
      "240/240 - 2s - loss: 0.0210 - mape: 136.1282 - lstm_me: 0.0017 - 2s/epoch - 7ms/step\n",
      "Epoch 84/150\n",
      "240/240 - 2s - loss: 0.0204 - mape: 181.4185 - lstm_me: -1.7159e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 85/150\n",
      "240/240 - 2s - loss: 0.0205 - mape: 117.7607 - lstm_me: 0.0014 - 2s/epoch - 7ms/step\n",
      "Epoch 86/150\n",
      "240/240 - 2s - loss: 0.0215 - mape: 240.4332 - lstm_me: 0.0012 - 2s/epoch - 7ms/step\n",
      "Epoch 87/150\n",
      "240/240 - 2s - loss: 0.0205 - mape: 145.8499 - lstm_me: -9.0636e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 88/150\n",
      "240/240 - 2s - loss: 0.0209 - mape: 135.5284 - lstm_me: 0.0041 - 2s/epoch - 7ms/step\n",
      "Epoch 89/150\n",
      "240/240 - 2s - loss: 0.0213 - mape: 136.0194 - lstm_me: 0.0016 - 2s/epoch - 7ms/step\n",
      "Epoch 90/150\n",
      "240/240 - 3s - loss: 0.0209 - mape: 354.9720 - lstm_me: -3.3355e-03 - 3s/epoch - 12ms/step\n",
      "Epoch 91/150\n",
      "240/240 - 3s - loss: 0.0220 - mape: 109.9218 - lstm_me: -1.5446e-03 - 3s/epoch - 12ms/step\n",
      "Epoch 92/150\n",
      "240/240 - 3s - loss: 0.0190 - mape: 138.0866 - lstm_me: 0.0074 - 3s/epoch - 12ms/step\n",
      "Epoch 93/150\n",
      "240/240 - 2s - loss: 0.0189 - mape: 147.7300 - lstm_me: -4.4937e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 94/150\n",
      "240/240 - 2s - loss: 0.0206 - mape: 234.0742 - lstm_me: 0.0023 - 2s/epoch - 8ms/step\n",
      "Epoch 95/150\n",
      "240/240 - 2s - loss: 0.0208 - mape: 115.7705 - lstm_me: -7.7017e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 96/150\n",
      "240/240 - 3s - loss: 0.0197 - mape: 204.6449 - lstm_me: -2.1597e-03 - 3s/epoch - 11ms/step\n",
      "Epoch 97/150\n",
      "240/240 - 4s - loss: 0.0194 - mape: 111.6661 - lstm_me: 0.0012 - 4s/epoch - 17ms/step\n",
      "Epoch 98/150\n",
      "240/240 - 2s - loss: 0.0191 - mape: 200.5475 - lstm_me: 0.0022 - 2s/epoch - 7ms/step\n",
      "Epoch 99/150\n",
      "240/240 - 2s - loss: 0.0170 - mape: 142.7956 - lstm_me: -3.1682e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 100/150\n",
      "240/240 - 2s - loss: 0.0164 - mape: 152.6517 - lstm_me: -4.8509e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 101/150\n",
      "240/240 - 2s - loss: 0.0190 - mape: 300.3906 - lstm_me: -9.0094e-05 - 2s/epoch - 8ms/step\n",
      "Epoch 102/150\n",
      "240/240 - 2s - loss: 0.0188 - mape: 120.5279 - lstm_me: 7.5897e-05 - 2s/epoch - 8ms/step\n",
      "Epoch 103/150\n",
      "240/240 - 2s - loss: 0.0194 - mape: 118.3707 - lstm_me: -1.0171e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 104/150\n",
      "240/240 - 2s - loss: 0.0179 - mape: 137.8359 - lstm_me: 0.0021 - 2s/epoch - 8ms/step\n",
      "Epoch 105/150\n",
      "240/240 - 2s - loss: 0.0182 - mape: 72.9814 - lstm_me: 3.3480e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 106/150\n",
      "240/240 - 2s - loss: 0.0183 - mape: 96.0629 - lstm_me: -1.6993e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 107/150\n",
      "240/240 - 2s - loss: 0.0195 - mape: 140.2986 - lstm_me: 4.4590e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 108/150\n",
      "240/240 - 2s - loss: 0.0176 - mape: 104.2141 - lstm_me: 0.0028 - 2s/epoch - 9ms/step\n",
      "Epoch 109/150\n",
      "240/240 - 2s - loss: 0.0169 - mape: 124.9305 - lstm_me: -2.0109e-03 - 2s/epoch - 9ms/step\n",
      "Epoch 110/150\n",
      "240/240 - 2s - loss: 0.0181 - mape: 163.6983 - lstm_me: 0.0086 - 2s/epoch - 8ms/step\n",
      "Epoch 111/150\n",
      "240/240 - 2s - loss: 0.0164 - mape: 93.9555 - lstm_me: -7.7995e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 112/150\n",
      "240/240 - 2s - loss: 0.0159 - mape: 78.9389 - lstm_me: 0.0021 - 2s/epoch - 8ms/step\n",
      "Epoch 113/150\n",
      "240/240 - 2s - loss: 0.0167 - mape: 156.7307 - lstm_me: -2.0521e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 114/150\n",
      "240/240 - 2s - loss: 0.0207 - mape: 127.8343 - lstm_me: 0.0048 - 2s/epoch - 8ms/step\n",
      "Epoch 115/150\n",
      "240/240 - 2s - loss: 0.0170 - mape: 140.5656 - lstm_me: 3.1483e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 116/150\n",
      "240/240 - 2s - loss: 0.0159 - mape: 70.8591 - lstm_me: -4.2091e-06 - 2s/epoch - 8ms/step\n",
      "Epoch 117/150\n",
      "240/240 - 2s - loss: 0.0168 - mape: 89.3029 - lstm_me: -3.4047e-03 - 2s/epoch - 9ms/step\n",
      "Epoch 118/150\n",
      "240/240 - 2s - loss: 0.0155 - mape: 103.5658 - lstm_me: 0.0045 - 2s/epoch - 8ms/step\n",
      "Epoch 119/150\n",
      "240/240 - 2s - loss: 0.0152 - mape: 97.9665 - lstm_me: 0.0014 - 2s/epoch - 8ms/step\n",
      "Epoch 120/150\n",
      "240/240 - 2s - loss: 0.0159 - mape: 55.2534 - lstm_me: -2.8577e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 121/150\n",
      "240/240 - 2s - loss: 0.0162 - mape: 165.2107 - lstm_me: -3.3322e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 122/150\n",
      "240/240 - 2s - loss: 0.0147 - mape: 63.1836 - lstm_me: 0.0030 - 2s/epoch - 9ms/step\n",
      "Epoch 123/150\n",
      "240/240 - 2s - loss: 0.0138 - mape: 82.2751 - lstm_me: 0.0017 - 2s/epoch - 8ms/step\n",
      "Epoch 124/150\n",
      "240/240 - 2s - loss: 0.0144 - mape: 105.9675 - lstm_me: 3.0889e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 125/150\n",
      "240/240 - 2s - loss: 0.0142 - mape: 67.4204 - lstm_me: -1.7756e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 126/150\n",
      "240/240 - 2s - loss: 0.0147 - mape: 147.1521 - lstm_me: 0.0024 - 2s/epoch - 8ms/step\n",
      "Epoch 127/150\n",
      "240/240 - 2s - loss: 0.0183 - mape: 158.0883 - lstm_me: 6.1246e-04 - 2s/epoch - 9ms/step\n",
      "Epoch 128/150\n",
      "240/240 - 2s - loss: 0.0153 - mape: 53.2453 - lstm_me: -4.5559e-03 - 2s/epoch - 8ms/step\n",
      "Epoch 129/150\n",
      "240/240 - 2s - loss: 0.0129 - mape: 82.4271 - lstm_me: 0.0026 - 2s/epoch - 8ms/step\n",
      "Epoch 130/150\n",
      "240/240 - 2s - loss: 0.0142 - mape: 140.3720 - lstm_me: -2.4256e-04 - 2s/epoch - 8ms/step\n",
      "Epoch 131/150\n",
      "240/240 - 2s - loss: 0.0141 - mape: 147.4852 - lstm_me: 0.0018 - 2s/epoch - 7ms/step\n",
      "Epoch 132/150\n",
      "240/240 - 2s - loss: 0.0140 - mape: 47.1575 - lstm_me: -1.8873e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 133/150\n",
      "240/240 - 2s - loss: 0.0121 - mape: 63.8635 - lstm_me: -1.2545e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 134/150\n",
      "240/240 - 2s - loss: 0.0130 - mape: 176.5598 - lstm_me: 0.0033 - 2s/epoch - 7ms/step\n",
      "Epoch 135/150\n",
      "240/240 - 2s - loss: 0.0112 - mape: 86.8048 - lstm_me: -2.1361e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 136/150\n",
      "240/240 - 2s - loss: 0.0107 - mape: 39.5174 - lstm_me: 1.5838e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 137/150\n",
      "240/240 - 2s - loss: 0.0139 - mape: 85.1953 - lstm_me: -4.2404e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 138/150\n",
      "240/240 - 2s - loss: 0.0113 - mape: 135.1247 - lstm_me: 0.0023 - 2s/epoch - 8ms/step\n",
      "Epoch 139/150\n",
      "240/240 - 2s - loss: 0.0118 - mape: 57.1552 - lstm_me: 4.3119e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 140/150\n",
      "240/240 - 2s - loss: 0.0121 - mape: 47.9946 - lstm_me: 5.3091e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 141/150\n",
      "240/240 - 2s - loss: 0.0122 - mape: 42.0586 - lstm_me: -1.8201e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 142/150\n",
      "240/240 - 2s - loss: 0.0103 - mape: 116.4270 - lstm_me: 0.0036 - 2s/epoch - 9ms/step\n",
      "Epoch 143/150\n",
      "240/240 - 2s - loss: 0.0108 - mape: 88.7490 - lstm_me: 0.0022 - 2s/epoch - 8ms/step\n",
      "Epoch 144/150\n",
      "240/240 - 2s - loss: 0.0114 - mape: 42.6121 - lstm_me: -3.3496e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 145/150\n",
      "240/240 - 2s - loss: 0.0122 - mape: 179.4922 - lstm_me: -5.2593e-04 - 2s/epoch - 9ms/step\n",
      "Epoch 146/150\n",
      "240/240 - 2s - loss: 0.0099 - mape: 93.6410 - lstm_me: -1.5960e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 147/150\n",
      "240/240 - 2s - loss: 0.0118 - mape: 105.9510 - lstm_me: 0.0027 - 2s/epoch - 7ms/step\n",
      "Epoch 148/150\n",
      "240/240 - 2s - loss: 0.0109 - mape: 116.9551 - lstm_me: -2.8145e-03 - 2s/epoch - 7ms/step\n",
      "Epoch 149/150\n",
      "240/240 - 2s - loss: 0.0112 - mape: 46.0231 - lstm_me: 0.0015 - 2s/epoch - 7ms/step\n",
      "Epoch 150/150\n",
      "240/240 - 2s - loss: 0.0099 - mape: 125.4328 - lstm_me: 2.8221e-04 - 2s/epoch - 8ms/step\n",
      "3/3 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11008\\3152948194.py:66: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plt.gca().set_yticklabels([f'{int(tick) / 1000000:.1f}' for tick in plt.gca().get_yticks()])\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(data, look_back=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i + look_back])\n",
    "        y.append(data[i + look_back])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def lstm_fit_predict_plot_evaluate(country_name, country_data):\n",
    "    if country_name == \"Cyprus\":\n",
    "        train_data = country_data[country_data.index > pd.to_datetime('2016-09-21', format='%Y-%m-%d')]\n",
    "    train_data = country_data[country_data.index <= pd.to_datetime('2019-10-22', format='%Y-%m-%d')]\n",
    "    test_data = country_data[country_data.index >= pd.to_datetime('2019-10-23', format='%Y-%m-%d')]\n",
    "    \n",
    "    look_back = 10\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train_data = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "    test_data_rescaled = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "    X_train, y_train = prepare_data(train_data, look_back)\n",
    "    X_test, y_test = prepare_data(test_data_rescaled, look_back)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(look_back, 1), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', metrics=[\"mape\", lstm_me], optimizer='adam')\n",
    "    \n",
    "    fit_model = model.fit(X_train, y_train, epochs=150, batch_size=1, verbose=2) \n",
    "\n",
    "    yhat = model.predict(X_test)\n",
    "    predictions_rescaled = scaler.inverse_transform(yhat)\n",
    "    yhat_df = pd.DataFrame(predictions_rescaled, index=test_data.index[look_back:])\n",
    "    model_filename = f\"lstm_model_{country_name}.pkl\"\n",
    "    model_file_path = os.path.join(models_output_folder, model_filename)\n",
    "\n",
    "    if not os.path.exists(model_file_path):\n",
    "        with open(model_file_path, 'wb') as model_file:\n",
    "            pickle.dump(fit_model, model_file)\n",
    "\n",
    "    country_data_subset = country_data.iloc[1:-1] \n",
    "\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.ylabel(\"Electricity Consumption (Terawatts)\", fontsize=14)\n",
    "    plt.title(f\"LSTM Prediction for Electricity Consumption in Terawatts for Country: {country_name}\", fontsize=16)\n",
    "    \n",
    "    country_data_subset = country_data.iloc[1:-1] \n",
    "    yhat_df_subset = yhat_df.iloc[1:-1]\n",
    "    \n",
    "    formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "    formatter.set_scientific(False)\n",
    "    formatter.set_powerlimits((-6, 6))\n",
    "    plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    plt.plot(yhat_df_subset, color=\"firebrick\", label='Predicted')\n",
    "    plt.plot(country_data_subset, color='steelblue', label='Actual')\n",
    "    \n",
    "    plt.xlim(country_data_subset.index[0], country_data_subset.index[-1])\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.gca().set_yticklabels([f'{int(tick) / 1000000:.1f}' for tick in plt.gca().get_yticks()])\n",
    "\n",
    "    output_file_path = os.path.join(images_output_folder, f\"lstm_predictions_{country}.png\")\n",
    "    if not os.path.exists(output_file_path):\n",
    "        plt.savefig(output_file_path)\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "    MAE_metric = round(mean_absolute_error(test_data.iloc[look_back:], yhat_df), 2)\n",
    "    MSE_metric = round(mean_squared_error(test_data.iloc[look_back:], yhat_df), 2)\n",
    "    RMSE_metric = RMSE(MSE_metric)\n",
    "\n",
    "    results = {\n",
    "        'Model': 'lstm',\n",
    "        'Country': country_name,\n",
    "        'MAE': MAE_metric,\n",
    "        'MSE': MSE_metric,\n",
    "        'RMSE': RMSE_metric\n",
    "    }\n",
    "    \n",
    "    return fit_model, results, yhat_df\n",
    "\n",
    "lstm_predictions = pd.DataFrame()\n",
    "lstm_fit_history_results = pd.DataFrame(columns=[\"Country\", \"Epoch\", \"Loss\", \"MAPE\", \"ME\"])\n",
    "results_list = []\n",
    "\n",
    "for country in country_list:\n",
    "    try: \n",
    "        print(f'Evaluation for country: {country}')\n",
    "        country_data = data_for_the_model[data_for_the_model['Country'] == country].set_index('Date').asfreq('W')\n",
    "        country_history, country_results, yhat_df = lstm_fit_predict_plot_evaluate(country, country_data[\"TotalLoad_Imputed_MW\"])\n",
    "        yhat_df = yhat_df.rename(columns={0: country})\n",
    "        lstm_predictions = pd.concat([lstm_predictions, yhat_df], axis=1)\n",
    "        locals()[f'history_{country}'] = country_history\n",
    "        epochs = country_history.epoch\n",
    "        loss = country_history.history['loss']\n",
    "        mape = country_history.history['mape']\n",
    "        me = country_history.history['lstm_me']\n",
    "\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"Country\": [country] * len(epochs),\n",
    "            \"Epoch\": epochs,\n",
    "            \"Loss\": loss,\n",
    "            \"MAPE\": mape,\n",
    "            \"ME\": me\n",
    "        })\n",
    "\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.ylabel(\"Mean Squared Error\", fontsize=14)\n",
    "        plt.title(f\"LSTM Loss Function: {country}\", fontsize=16)\n",
    "\n",
    "        formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(False)\n",
    "        formatter.set_powerlimits((-6, 6))\n",
    "        plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "        # Loss function generation\n",
    "        plt.plot(temp_df[\"Epoch\"], temp_df[\"Loss\"], color=\"steelblue\", label='Loss Function')\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        output_file_path = os.path.join(images_output_folder, f\"lstm_loss_function_{country}.png\")\n",
    "        if not os.path.exists(output_file_path):\n",
    "            plt.savefig(output_file_path)\n",
    "    \n",
    "        plt.close()\n",
    "        \n",
    "        lstm_fit_history_results = pd.concat([lstm_fit_history_results, temp_df], ignore_index=True)\n",
    "        results_list.append(country_results)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for country {country}: {e}\")\n",
    "\n",
    "lstm_results_without_me_mape = pd.DataFrame(results_list)\n",
    "lstm_me_mape = lstm_fit_history_results[lstm_fit_history_results[\"Epoch\"] == 149].drop(columns=[\"Epoch\", \"Loss\"])\n",
    "lstm_results = lstm_results_without_me_mape.merge(lstm_me_mape, on=\"Country\")\n",
    "\n",
    "lstm_fit_history_results.to_excel(os.path.join(tables_output_folder, \"tab_10.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WYNIKI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = pd.concat([sarima_results, tbats_results, lstm_results], ignore_index=True).sort_values(by=['Country', 'Model'], ascending=[True, True])\n",
    "\n",
    "evaluation_results.to_excel(os.path.join(tables_output_folder, \"tab_11.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procentowe różnica między wartościami rzeczywistymi a prognozowanymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_predictions_copy = sarima_predictions.copy(deep=True)\n",
    "sarima_predictions_copy.reset_index(inplace=True)\n",
    "sarima_predictions_copy['Date'] = sarima_predictions_copy['index'] - pd.Timedelta(days=1)\n",
    "sarima_predictions_copy = sarima_predictions_copy.drop(sarima_predictions_copy.index[0])\n",
    "sarima_predictions_copy = sarima_predictions_copy.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbats_predictions_copy = tbats_predictions.copy(deep=True)\n",
    "tbats_predictions_copy.reset_index(inplace=True)\n",
    "tbats_predictions_copy['Date'] = tbats_predictions_copy['index'] + pd.Timedelta(days=4)\n",
    "tbats_predictions_copy = tbats_predictions_copy.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions_copy = lstm_predictions.copy(deep=True)\n",
    "lstm_predictions_copy.reset_index(inplace=True)\n",
    "lstm_predictions_copy['Date'] = lstm_predictions_copy['Date'] - pd.Timedelta(days=1)\n",
    "lstm_predictions_copy = lstm_predictions_copy.drop(lstm_predictions_copy.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Date</th>\n",
       "      <th>TotalLoad_Imputed_MW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>4705736.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>5261169.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>5629494.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>5609838.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>5348570.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country       Date  TotalLoad_Imputed_MW\n",
       "260  Austria 2020-01-05            4705736.00\n",
       "261  Austria 2020-01-12            5261169.00\n",
       "262  Austria 2020-01-19            5629494.00\n",
       "263  Austria 2020-01-26            5609838.00\n",
       "264  Austria 2020-02-02            5348570.00"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data = data_for_the_model[data_for_the_model[\"Date\"] >= pd.to_datetime('2020-01-01', format='%Y-%m-%d')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dfs = {}\n",
    "max_percentage_errors = {'Country': [], 'SARIMA': [], 'TBATS': [], 'LSTM': []}\n",
    "mean_percentage_errors = {'Country': [], 'SARIMA': [], 'TBATS': [], 'LSTM': []}\n",
    "\n",
    "for country in country_list:\n",
    "\n",
    "    data = actual_data[actual_data['Country'] == country].set_index('Date').asfreq('W').drop(columns=['Country']).rename(\n",
    "        columns={'TotalLoad_Imputed_MW': 'Actual'})\n",
    "    country_df = data.copy().reset_index()\n",
    "    \n",
    "    sarima_preds = sarima_predictions_copy[['Date', country]]\n",
    "    tbats_preds = tbats_predictions_copy[['Date', country]]\n",
    "    lstm_preds = lstm_predictions_copy[['Date', country]]\n",
    "    \n",
    "    country_df.reset_index(drop=True, inplace=True)\n",
    "    sarima_preds.reset_index(drop=True, inplace=True)\n",
    "    tbats_preds.reset_index(drop=True, inplace=True)\n",
    "    lstm_preds.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    country_df[\"SARIMA\"] = sarima_preds[country]\n",
    "    country_df[\"SARIMA_percentage_error\"] = round(abs(100 * (1 - country_df['Actual'] / country_df['SARIMA'])), 2)\n",
    "    country_df[\"TBATS\"] = tbats_preds[country]\n",
    "    country_df[\"TBATS_percentage_error\"] = round(abs(100 * (1 - country_df['Actual'] / country_df['TBATS'])), 2)\n",
    "    country_df[\"LSTM\"] = lstm_preds[country]\n",
    "    country_df[\"LSTM_percentage_error\"] = round(abs(100 * (1 - country_df['Actual'] / country_df['LSTM'])), 2)\n",
    "\n",
    "    max_percentage_errors['Country'].append(country)\n",
    "    max_percentage_errors['SARIMA'].append(country_df['SARIMA_percentage_error'].max())\n",
    "    max_percentage_errors['TBATS'].append(country_df['TBATS_percentage_error'].max())\n",
    "    max_percentage_errors['LSTM'].append(country_df['LSTM_percentage_error'].max())\n",
    "\n",
    "    mean_percentage_errors['Country'].append(country)\n",
    "    mean_percentage_errors['SARIMA'].append(country_df['SARIMA_percentage_error'].mean())\n",
    "    mean_percentage_errors['TBATS'].append(country_df['TBATS_percentage_error'].mean())\n",
    "    mean_percentage_errors['LSTM'].append(country_df['LSTM_percentage_error'].mean())\n",
    "    \n",
    "    country_df.set_index(\"Date\", inplace=True)\n",
    "    \n",
    "    result_dfs[f\"{country}_predictions\"] = country_df\n",
    "\n",
    "max_errors_df = pd.DataFrame(max_percentage_errors)\n",
    "max_errors_df['SARIMA'] = max_errors_df['SARIMA'].apply(lambda x: '{:.2f}'.format(x))\n",
    "max_errors_df['TBATS'] = max_errors_df['TBATS'].apply(lambda x: '{:.2f}'.format(x))\n",
    "max_errors_df['LSTM'] = max_errors_df['LSTM'].apply(lambda x: '{:.2f}'.format(x))\n",
    "max_errors_df.style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_12.tex\"), hrules=True)\n",
    "\n",
    "mean_errors_df = pd.DataFrame(mean_percentage_errors)\n",
    "mean_errors_df['SARIMA'] = mean_errors_df['SARIMA'].apply(lambda x: '{:.2f}'.format(x))\n",
    "mean_errors_df['TBATS'] = mean_errors_df['TBATS'].apply(lambda x: '{:.2f}'.format(x))\n",
    "mean_errors_df['LSTM'] = mean_errors_df['LSTM'].apply(lambda x: '{:.2f}'.format(x))\n",
    "mean_errors_df.style.hide(axis = 0).to_latex(os.path.join(tables_output_folder, \"tab_13.tex\"), hrules=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykresy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procentowe różnice między wartościami rzeczywistymi a prognozowanymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "poland = result_dfs[\"Poland_predictions\"]\n",
    "poland_errors = poland[[\"SARIMA_percentage_error\", \"TBATS_percentage_error\", \"LSTM_percentage_error\"]].reset_index()\n",
    "poland_errors = poland_errors.rename(columns={\n",
    "    \"SARIMA_percentage_error\": \"SARIMA\",\n",
    "    \"TBATS_percentage_error\": \"TBATS\", \n",
    "    \"LSTM_percentage_error\": \"LSTM\"\n",
    "})\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.ylabel(\"Prediction error (%)\", fontsize=14)\n",
    "plt.title(f\"Temporal Analysis of Prediction Errors in SARIMA, TBATS, and LSTM Models for Country: Poland\", fontsize=16)\n",
    "    \n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "formatter.set_powerlimits((-6, 6))\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "plt.plot(poland_errors[\"Date\"], poland_errors[\"SARIMA\"], color='darkblue', label='SARIMA')\n",
    "plt.plot(poland_errors[\"Date\"], poland_errors[\"TBATS\"], color='blue', label='TBATS')\n",
    "plt.plot(poland_errors[\"Date\"], poland_errors[\"LSTM\"], color='steelblue', label='LSTM')\n",
    "    \n",
    "plt.xlim(poland_errors[\"Date\"].iloc[0], poland_errors[\"Date\"].iloc[-1])\n",
    "plt.legend()\n",
    "    \n",
    "ticks = plt.gca().get_yticks()\n",
    "tick_labels = [f'{int(tick) / 1000000:.1f}' for tick in ticks]\n",
    "plt.gca().yaxis.set_major_locator(ticker.FixedLocator(ticks))\n",
    "plt.gca().set_yticklabels(tick_labels)\n",
    "\n",
    "output_file_path = os.path.join(images_output_folder, f\"maximum_percentage_error.png\")\n",
    "if not os.path.exists(output_file_path):\n",
    "    plt.savefig(output_file_path)\n",
    "    \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Połączone wykresy porównujące modele dla SARIMA, TBATS i LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the models\n",
    "models = ['sarima', 'tbats', 'lstm']\n",
    "\n",
    "for country in country_list:\n",
    "    # List of paths to PNG files with plots for each model\n",
    "    paths = [os.path.join(images_output_folder, f'{model}_predictions_{country}.png') for model in models]\n",
    "\n",
    "    # Load PNG images\n",
    "    images = [Image.open(path) for path in paths]\n",
    "\n",
    "    # Get image dimensions\n",
    "    widths, heights = zip(*(image.size for image in images))\n",
    "\n",
    "    # Create a new image with a width equal to the widest image and a height equal to the sum of heights\n",
    "    new_width = max(widths)\n",
    "    new_height = sum(heights)\n",
    "\n",
    "    new_image = Image.new('RGB', (new_width, new_height), (255, 255, 255))  # White background\n",
    "\n",
    "    # Paste images onto the new image\n",
    "    current_height = 0\n",
    "    for image in images:\n",
    "        new_image.paste(image, (0, current_height))\n",
    "        current_height += image.size[1]\n",
    "\n",
    "    # Save the new image\n",
    "    output_file_path = os.path.join(images_output_folder, f'model_comparison_{country}.png')\n",
    "    new_image.save(output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
